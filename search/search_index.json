{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Voice Agent","text":"<p>Welcome to the Voice Agent documentation. This is an event-driven, multi-service voice agent system that integrates speech-to-text (STT), language models (LLM), and text-to-speech (TTS) to create an intelligent voice assistant.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Microservices Architecture: Decoupled services for STT, TTS, and Orchestration.</li> <li>Event-Driven: Internal asynchronous event bus for low-latency interaction.</li> <li>Real-Time Streaming: Streaming support for STT transcripts, LLM tokens, and TTS audio.</li> <li>Provider System: Extensible support for multiple AI providers (Ollama, Gemini, FunASR, Edge-TTS, etc.).</li> <li>Voice Activity Detection (VAD): Smart microphone capture that only streams when you speak.</li> <li>Interruption Support: Seamlessly interrupt the agent while it's speaking.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>uv (recommended for dependency management)</li> <li>CUDA-capable GPU (recommended)</li> <li><code>ffmpeg</code> (required for Edge TTS)</li> </ul>"},{"location":"#2-installation","title":"2. Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/ndwang/voice_agent.git\ncd voice_agent\n\n# Install dependencies\nuv sync\n</code></pre> <p>Note: The default setup assumes CUDA 12.6. Optional components like blivedm, ChatTTS, Genie TTS, and Edge-TTS require separate installation or system setup. See the Installation Guide for details.</p>"},{"location":"#3-run-all-services","title":"3. Run All Services","text":"<p>The easiest way to start the system is using the provided script:</p> <pre><code>uv run scripts/start_services.py\n</code></pre> <p>This will launch: 1. STT Service (Port 8001) 2. TTS Service (Port 8003) 3. Orchestrator (Port 8000)</p>"},{"location":"#4-talk-to-the-agent","title":"4. Talk to the Agent","text":"<p>Once the services are running: - Speak: The agent will listen and transcribe your voice. - Toggle Listening: Use <code>Ctrl+Shift+L</code> to enable/disable the microphone. - Web UI: Access the control panel at <code>http://localhost:8000/ui</code>.</p> <p>See the Architecture section for a deep dive into how it works.</p>"},{"location":"architecture/event-system/","title":"Event-Driven Core","text":"<p>At the heart of the Orchestrator is the <code>EventBus</code>. This internal mechanism allows for decoupled communication between \"Sources\" (which produce events) and \"Managers\" (which consume and act on events).</p>"},{"location":"architecture/event-system/#the-event-bus","title":"The Event Bus","text":"<p>The <code>EventBus</code> is an asynchronous implementation that supports: - Subscribe: Register an async callback for a specific <code>EventType</code>. - Publish: Broadcast an <code>Event</code> to all registered subscribers. - Concurrency: Callbacks are executed as independent tasks, ensuring that a slow subscriber (like a logger) doesn't block the main interaction flow.</p>"},{"location":"architecture/event-system/#common-event-types","title":"Common Event Types","text":"<p>The system defines a standard set of events in <code>orchestrator/events.py</code>:</p> Event Category Type Description Input <code>input.speech_start</code> User started speaking (useful for interruption). <code>input.transcript.final</code> Finalized sentence from STT. <code>input.transcript.interim</code> Real-time partial transcript from STT. LLM <code>llm.request</code> Prompt sent to the LLM. <code>llm.token</code> A single token generated by the LLM. <code>llm.response_done</code> LLM generation is complete. <code>llm.cancelled</code> LLM generation was aborted. TTS <code>tts.request</code> Text sent to the TTS service for synthesis. <code>tts.audio_chunk</code> Raw PCM audio received from TTS. System <code>system.state_changed</code> Updates to <code>listening</code>, <code>responding</code>, or <code>playing</code> states."},{"location":"architecture/event-system/#why-event-driven","title":"Why Event-Driven?","text":"<ol> <li>Low Latency: As soon as a \"Source\" gets data (like a finalized transcript), it fires an event. The <code>InteractionManager</code> starts the LLM request immediately while other components (like the UI) update simultaneously.</li> <li>Extensibility: Want to add OBS subtitles? Simply create a <code>SubtitleManager</code> that subscribes to <code>tts.request</code> or <code>llm.token</code>. You don't need to modify the core interaction logic.</li> <li>Robustness: If one subscriber fails, the <code>EventBus</code> catches the exception and continues notifying others.</li> </ol>"},{"location":"architecture/interaction-flow/","title":"Interaction Flow &amp; Logic","text":"<p>The <code>InteractionManager</code> is responsible for the main loop of the voice agent. It translates user speech into agent responses by orchestrating the flow between STT, LLM, and TTS.</p>"},{"location":"architecture/interaction-flow/#the-interaction-pipeline","title":"The Interaction Pipeline","text":"<p>The standard flow follows these steps:</p> <ol> <li>Transcript Finalized: <code>STTSource</code> publishes <code>TRANSCRIPT_FINAL</code>.</li> <li>Context Preparation: <code>InteractionManager</code> asks <code>ContextManager</code> for the conversation history + system prompt.</li> <li>LLM Generation: The prompt is sent to the LLM provider. Tokens are streamed back.</li> <li>Stream Parsing: Tokens are passed through a parser.</li> <li>TTS Request: When a complete sentence or tagged block is identified, a <code>TTS_REQUEST</code> is published.</li> <li>Audio Playback: <code>TTSManager</code> receives the request, streams it to the TTS Service, and sends the resulting audio chunks to the <code>AudioPlayer</code>.</li> </ol>"},{"location":"architecture/interaction-flow/#standard-flow-sequence-diagram","title":"Standard Flow Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant AD as Audio Driver\n    participant STT_SVC as STT Service\n    participant STT_SRC as STT Source\n    participant EB as Event Bus\n    participant IM as Interaction Manager\n    participant CM as Context Manager\n    participant LLM as LLM Provider\n    participant Parser as Stream Parser\n    participant TTS_M as TTS Manager\n    participant TTS_SVC as TTS Service\n    participant AP as Audio Player\n\n    U-&gt;&gt;AD: Speaks\n    AD-&gt;&gt;STT_SVC: Audio stream (WebSocket)\n    STT_SVC-&gt;&gt;STT_SRC: Transcript\n    STT_SRC-&gt;&gt;EB: Event: TRANSCRIPT_FINAL\n    EB-&gt;&gt;IM: Event: TRANSCRIPT_FINAL\n\n    IM-&gt;&gt;CM: Get conversation history + system prompt\n    CM--&gt;&gt;IM: Context (messages + system prompt)\n\n    IM-&gt;&gt;LLM: Generate stream (messages, system_prompt)\n\n    loop For each token\n        LLM--&gt;&gt;IM: Stream token\n        IM-&gt;&gt;EB: Event: LLM_TOKEN (for UI)\n        IM-&gt;&gt;Parser: Process token\n        Parser-&gt;&gt;Parser: Buffer until sentence complete\n\n        opt When sentence or tagged block complete\n            Parser-&gt;&gt;IM: Sentence ready\n            IM-&gt;&gt;EB: Event: TTS_REQUEST (text)\n            EB-&gt;&gt;TTS_M: Event: TTS_REQUEST\n\n            TTS_M-&gt;&gt;TTS_SVC: Text chunk (WebSocket)\n            TTS_SVC--&gt;&gt;TTS_M: Audio chunk\n            TTS_M-&gt;&gt;AP: Play audio chunk\n            AP-&gt;&gt;U: Audio output\n        end\n    end\n\n    LLM--&gt;&gt;IM: Stream complete\n    IM-&gt;&gt;EB: Event: LLM_RESPONSE_DONE\n    EB-&gt;&gt;TTS_M: Event: LLM_RESPONSE_DONE\n    TTS_M-&gt;&gt;TTS_SVC: Finalize stream\n    TTS_SVC--&gt;&gt;TTS_M: Final audio chunks\n    TTS_M-&gt;&gt;AP: Play remaining audio\n    AP-&gt;&gt;U: Audio output</code></pre>"},{"location":"architecture/interaction-flow/#interruption-logic","title":"Interruption Logic","text":"<p>One of the most complex parts of the system is handling interruptions (when the user starts speaking while the agent is still responding).</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant AD as Audio Driver\n    participant STT as STT Service\n    participant IM as Interaction Manager\n    participant TTS as TTS Manager\n\n    IM-&gt;&gt;TTS: Playing response...\n    U-&gt;&gt;AD: (User starts speaking)\n    AD-&gt;&gt;STT: Audio stream\n    STT--&gt;&gt;IM: Event: SPEECH_START\n    IM-&gt;&gt;IM: Cancel current LLM generation\n    IM-&gt;&gt;TTS: Stop playback &amp; Clear queue\n    STT--&gt;&gt;IM: Event: TRANSCRIPT_FINAL\n    IM-&gt;&gt;IM: Start new interaction</code></pre>"},{"location":"architecture/interaction-flow/#concatenation-logic","title":"Concatenation Logic","text":"<p>If the user interrupts the agent before it finishes its sentence, the system can be configured to concatenate the interrupted prompt with the next user input, ensuring the agent understands the context of the unfinished thought. Currently this is commented out.</p>"},{"location":"architecture/overview/","title":"System Architecture Overview","text":"<p>The Voice Agent is built on a Microservices Architecture using an Event-Driven core. This design ensures that each component (STT, TTS, LLM) can operate independently and asynchronously, providing a responsive experience.</p>"},{"location":"architecture/overview/#high-level-diagram","title":"High-Level Diagram","text":"<pre><code>graph TD\n    subgraph Client [Client Side]\n        AD[Audio Driver]\n        UI[Web UI]\n    end\n\n    subgraph Orchestrator [Orchestrator - Port 8000]\n        EB[Event Bus]\n        IM[Interaction Manager]\n        CM[Context Manager]\n        STT_S[STT Source]\n        TTS_M[TTS Manager]\n    end\n\n    subgraph Services [Back-end Services]\n        STT_SVC[STT Service - Port 8001]\n        TTS_SVC[TTS Service - Port 8003]\n        LLM[LLM Provider - Ollama/Gemini]\n    end\n\n    AD -- Audio Stream --&gt; STT_SVC\n    STT_SVC -- Transcripts --&gt; STT_S\n    STT_S -- Event --&gt; EB\n    EB -- Event --&gt; IM\n    IM -- Request --&gt; LLM\n    LLM -- Stream --&gt; IM\n    IM -- Event --&gt; EB\n    EB -- Event --&gt; TTS_M\n    TTS_M -- Text Stream --&gt; TTS_SVC\n    TTS_SVC -- Audio Chunks --&gt; TTS_M\n    TTS_M -- Play Audio --&gt; AD</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-orchestrator","title":"1. Orchestrator","text":"<p>The \"brain\" of the system. It doesn't perform heavy AI processing itself but coordinates between services. It maintains the Event Bus, manages conversation context, and handles the logic for starting/stopping interactions.</p>"},{"location":"architecture/overview/#2-event-bus","title":"2. Event Bus","text":"<p>A simple, internal pub/sub system. Every major action (transcript received, user spoke, LLM generated a token, TTS finished) is an Event. This allows components like the <code>SubtitleManager</code> or <code>LatencyTracker</code> to \"listen\" to the system without being tightly coupled to the main logic.</p>"},{"location":"architecture/overview/#3-audio-driver","title":"3. Audio Driver","text":"<p>A local component that handles hardware interaction. It uses Voice Activity Detection (VAD) to capture microphone input only when speech is detected and handles the real-time playback of audio chunks received from the TTS service.</p>"},{"location":"architecture/overview/#4-ai-services-stt-tts","title":"4. AI Services (STT &amp; TTS)","text":"<p>Stateless services that wrap specific AI models. They expose WebSocket APIs for real-time streaming, allowing the Orchestrator to pipe text or audio through them with minimal overhead.</p>"},{"location":"components/audio_player/","title":"Audio Player Service","text":"<p>The Audio Player provides low-latency streaming audio playback for the voice agent system. It is designed to play audio chunks in real-time as they are generated by the TTS service, enabling responsive voice interactions.</p>"},{"location":"components/audio_player/#features","title":"Features","text":"<ul> <li>Streaming Playback: Plays audio chunks as they arrive, without waiting for the complete audio stream.</li> <li>Low Latency: Optimized for minimal delay between audio generation and playback (typically ~50ms buffer).</li> <li>Automatic Resampling: Handles sample rate conversion between TTS output and playback device requirements.</li> <li>Device Selection: Supports custom audio output device selection via configuration.</li> <li>State Management: Provides callbacks for play state changes, enabling coordination with other system components.</li> </ul>"},{"location":"components/audio_player/#architecture","title":"Architecture","text":"<p>The AudioPlayer uses a multi-threaded architecture:</p> <ol> <li>Main Thread: Receives audio chunks via <code>play_audio_chunk()</code> and queues them for processing.</li> <li>Playback Loop: An async task that processes the queue, resamples audio, and feeds a buffer.</li> <li>Audio Thread: A separate thread (managed by sounddevice) that continuously reads from the buffer and outputs to the audio device.</li> </ol> <p>This design ensures that audio playback is never blocked by other operations, and incoming audio chunks can be processed immediately.</p> <pre><code>sequenceDiagram\n    participant Q as audio_queue\n    participant L as _playback_loop\n    participant B as _audio_buffer\n    participant C as _audio_callback (Thread)\n\n    L-&gt;&gt;Q: await get()\n    Q--&gt;&gt;L: item arrives\n    L-&gt;&gt;B: push data\n    L-&gt;&gt;L: set _audio_active = True\n    Note over L: notify on_play_state(True)\n\n    loop Playback\n        C-&gt;&gt;B: pop data\n        B--&gt;&gt;C: chunk\n        Note over C: output to speakers\n    end\n\n    C-&gt;&gt;B: pop data (empty!)\n    B--&gt;&gt;C: empty\n    C-&gt;&gt;L: call_soon_threadsafe(event.set)\n    Note over L: wake up immediately\n    L-&gt;&gt;L: set _audio_active = False\n    Note over L: notify on_play_state(False)</code></pre>"},{"location":"components/audio_player/#configuration","title":"Configuration","text":"<p>Audio Player settings are configured in <code>config.yaml</code> under the <code>audio.output</code> section:</p> <pre><code>audio:\n  output:\n    sample_rate: 32000  # Playback sample rate (Hz)\n    channels: 1         # Number of audio channels (1 = mono, 2 = stereo)\n    device: null        # Output device index or name (null = system default)\n</code></pre>"},{"location":"components/audio_player/#sample-rate","title":"Sample Rate","text":"<p>The playback sample rate should match your audio device's capabilities. Common values: - 16000 Hz: Standard for voice/speech - 32000 Hz: Higher quality for voice - 44100 Hz: CD quality - 48000 Hz: Professional audio standard</p>"},{"location":"components/audio_player/#device-selection","title":"Device Selection","text":"<p>To use a specific audio output device:</p> <ol> <li> <p>List available devices using the provided script:    <pre><code>uv run scripts/list_output_devices.py\n</code></pre></p> </li> <li> <p>Set the device in <code>config.yaml</code>:</p> </li> <li>By index: <code>device: 1</code></li> <li>By name: <code>device: \"Speakers (Realtek Audio)\"</code></li> <li>System default: <code>device: null</code></li> </ol>"},{"location":"components/audio_player/#audio-format","title":"Audio Format","text":""},{"location":"components/audio_player/#input-format","title":"Input Format","text":"<ul> <li>Data Type: <code>bytes</code> containing <code>float32</code> samples</li> <li>Normalization: Samples should be in the range <code>[-1.0, 1.0]</code></li> <li>Layout: Interleaved samples (for multi-channel audio)</li> <li>Sample Rate: Any rate supported by the source (TTS service typically outputs 24000 Hz)</li> </ul>"},{"location":"components/audio_player/#processing-pipeline","title":"Processing Pipeline","text":"<ol> <li>Queue: Incoming chunks are added to an async queue</li> <li>Conversion: Bytes are converted to NumPy arrays</li> <li>Resampling: Audio is resampled to the playback sample rate (if needed)</li> <li>Buffering: Resampled audio is added to a thread-safe buffer</li> <li>Playback: The audio device callback reads from the buffer in real-time</li> </ol>"},{"location":"components/audio_player/#resampling","title":"Resampling","text":"<p>The AudioPlayer automatically handles sample rate conversion using <code>scipy.signal.resample()</code> for high-quality resampling. Resampling is performed on-the-fly, so there's no need to pre-convert audio to the playback sample rate.</p>"},{"location":"components/audio_player/#latency-optimization","title":"Latency Optimization","text":"<p>The AudioPlayer is optimized for low latency:</p> <ul> <li>Small Buffer: ~50ms buffer size minimizes delay</li> <li>Streaming: Audio starts playing as soon as the first chunk arrives</li> <li>No Pre-buffering: The system doesn't wait for complete audio before starting playback</li> </ul> <p>This enables natural conversation flow, as the agent can start speaking almost immediately after generating the first audio chunk.</p>"},{"location":"components/audio_player/#thread-safety","title":"Thread Safety","text":"<p>The AudioPlayer is designed to be thread-safe:</p> <ul> <li>Queue Operations: The async queue handles concurrent access safely</li> <li>Buffer Access: Thread locks protect the audio buffer during read/write operations</li> <li>State Management: Play state changes are synchronized via the event loop</li> </ul> <p>You can safely call <code>play_audio_chunk()</code> from any async context without additional synchronization.</p>"},{"location":"components/audio_player/#error-handling","title":"Error Handling","text":"<p>The AudioPlayer includes robust error handling:</p> <ul> <li>Device Errors: Logs warnings if the audio device reports issues</li> <li>Processing Errors: Catches and logs exceptions during audio processing</li> <li>Stream Errors: Gracefully handles stream closure and cleanup</li> </ul> <p>If an error occurs, the playback loop will stop and clean up resources, but the player can be reused by calling <code>play_audio_chunk()</code> again.</p>"},{"location":"components/audio_player/#integration-with-tts-service","title":"Integration with TTS Service","text":"<p>The AudioPlayer is typically used by the Orchestrator's <code>TTSManager</code> to play audio received from the TTS service:</p> <ol> <li>TTS service generates audio chunks via WebSocket</li> <li>TTSManager receives chunks and calls <code>player.play_audio_chunk()</code></li> <li>AudioPlayer streams the audio to the output device</li> <li>Play state callbacks notify the system when playback starts/stops</li> </ol> <p>This integration enables seamless voice interactions with minimal latency.</p>"},{"location":"components/stt/","title":"STT Service (Speech-to-Text)","text":"<p>The STT Service provides real-time transcription of audio streams. It is designed to run as a standalone service (on port 8001) that multiple clients can connect to via WebSocket.</p>"},{"location":"components/stt/#supported-providers","title":"Supported Providers","text":""},{"location":"components/stt/#1-funasr-recommended","title":"1. FunASR (Recommended)","text":"<p>Uses Alibaba's FunASR framework. It is highly optimized for Chinese and English, supporting: - Streaming Paraformer: Low-latency transcription. - VAD (Voice Activity Detection): Built-in model to detect speech boundaries. - Punctuation: Automatic punctuation restoration.</p>"},{"location":"components/stt/#2-faster-whisper","title":"2. Faster-Whisper","text":"<p>An efficient implementation of OpenAI's Whisper model using CTranslate2.  - Best for high-accuracy multilingual support. - Supports different model sizes (tiny, base, small, medium, large).</p>"},{"location":"components/stt/#websocket-protocol","title":"WebSocket Protocol","text":""},{"location":"components/stt/#input-audio-streaming","title":"Input: Audio Streaming","text":"<p>The client sends raw PCM audio bytes (typically 16kHz, mono, float32) directly over the WebSocket.</p>"},{"location":"components/stt/#output-json-transcripts","title":"Output: JSON Transcripts","text":"<p>The service broadcasts transcripts to all connected clients:</p> <pre><code>// Interim (Partial) Result\n{\n  \"type\": \"interim\",\n  \"text\": \"Hello world\"\n}\n\n// Final (Sentence) Result\n{\n  \"type\": \"final\",\n  \"text\": \"Hello world.\"\n}\n\n// Speech Boundary Detection\n{\n  \"type\": \"speech_start\"\n}\n</code></pre>"},{"location":"components/stt/#internal-workflow","title":"Internal Workflow","text":"<ol> <li>VAD Chunking: Incoming audio is processed by a VAD model to identify speech segments.</li> <li>Model Inference: Speech chunks are sent to the ASR model.</li> <li>Post-Processing: (Optional) Punctuation and text formatting are applied.</li> <li>Broadcast: The resulting text is sent back through the WebSocket.</li> </ol>"},{"location":"components/tts/","title":"TTS Service (Text-to-Speech)","text":"<p>The TTS Service (port 8003) converts text into natural-sounding speech. It supports both high-quality offline models and fast online providers.</p>"},{"location":"components/tts/#supported-providers","title":"Supported Providers","text":""},{"location":"components/tts/#1-genie-tts-high-quality","title":"1. Genie-TTS (High Quality)","text":"<p>A high-performance CPU inference engine for GPT-SoVITS models.  - Character Consistency: Uses pre-trained character models. - Language Support: Optimized for Japanese, Chinese, and English. - Latency: GENIE optimizes the original model for outstanding CPU performance.</p>"},{"location":"components/tts/#2-edge-tts-fast-easy","title":"2. Edge-TTS (Fast &amp; Easy)","text":"<p>Uses Microsoft Edge's online TTS engine. - No GPU Required: Processing happens on Microsoft servers. - Wide Voice Selection: Access to dozens of neural voices. - Requirement: <code>ffmpeg</code> must be installed locally for format conversion.</p>"},{"location":"components/tts/#3-chattts","title":"3. ChatTTS","text":"<p>Optimized for conversational speech, including fillers like [laugh] and [um].</p>"},{"location":"components/tts/#websocket-streaming-protocol","title":"WebSocket Streaming Protocol","text":"<p>To minimize \"Time to First Byte\" (TTFB), the TTS Service supports streaming synthesis:</p> <ol> <li>Client Request:    <pre><code>{\n  \"type\": \"text\",\n  \"text\": \"Hello, how are you?\",\n  \"finalize\": false\n}\n</code></pre></li> <li>Audio Streaming: The service begins generating audio and sends it back as raw binary PCM chunks.</li> <li>Finalization: When <code>finalize: true</code> is sent, the service finishes the current buffer and closes the stream segment.</li> </ol>"},{"location":"components/tts/#latency-optimization","title":"Latency Optimization","text":"<ul> <li>Pre-connection: The Orchestrator's <code>TTSManager</code> opens a WebSocket connection as soon as the LLM starts generating, reducing the handshake overhead when the first sentence is ready.</li> <li>Sentence-Level Chunking: The system doesn't wait for the full LLM response. As soon as a sentence is complete (detected by punctuation or tags), it is sent to the TTS service.</li> </ul>"},{"location":"guide/configuration/","title":"Configuration Reference","text":"<p>The system is configured via <code>config.yaml</code> in the root directory. All services share this file, allowing for centralized management.</p>"},{"location":"guide/configuration/#service-ports","title":"Service Ports","text":"Service Port Description Orchestrator 8000 Main coordinator and Web UI STT Service 8001 Speech-to-Text WebSocket API LLM Service 8002 Language Model API TTS Service 8003 Text-to-Speech WebSocket API"},{"location":"guide/configuration/#core-configuration","title":"Core Configuration","text":""},{"location":"guide/configuration/#orchestrator","title":"Orchestrator","text":"<pre><code>orchestrator:\n  host: \"0.0.0.0\"\n  port: 8000\n  stt_websocket_path: \"/ws/stt\"\n  log_level: \"INFO\"\n  enable_latency_tracking: true\n  system_prompt_file: \"orchestrator/system_prompt.txt\"\n  hotkeys:\n    toggle_listening: \"ctrl+shift+l\"\n    cancel_speech: \"ctrl+shift+c\"\n</code></pre>"},{"location":"guide/configuration/#llm-providers","title":"LLM Providers","text":"<p>Supported: <code>ollama</code>, <code>gemini</code>, <code>llamacpp</code>.</p> <pre><code>llm:\n  provider: \"ollama\"  # Options: gemini, ollama\n  providers:\n    gemini:\n      model: \"gemini-2.5-flash\"\n      api_key: \"\"  # Optional: Override GEMINI_API_KEY env var\n      generation_config:\n        thinking_budget: 0  # 0 = disabled, -1 = dynamic (Gemini 2.5), 2.5 pro 128-32768, 2.5 flash 0-24576\n        # thinking_level: \"low\"  # Gemini 3 Pro low/high, Flash low/high/medium/minimal\n        temperature: 1.0\n        top_p: 0.95\n        top_k: 40\n        max_output_tokens: 8192\n    ollama:\n      model: \"Qwen3-8B-Q4-8kcontext\"\n      base_url: \"http://localhost:11434\"\n      timeout: 300.0\n      disable_thinking: true\n      generation_config:\n        temperature: 0.7\n        top_p: 0.9\n        top_k: 40\n        num_predict: 2048\n    llamacpp:\n      model_path: \"\"\n      n_ctx: 4096\n      n_threads: 0\n      n_gpu_layers: -1\n</code></pre>"},{"location":"guide/configuration/#stt-configuration","title":"STT Configuration","text":"<p>Supported: <code>funasr</code> (recommended), <code>faster-whisper</code>.</p> <pre><code>stt:\n  host: \"0.0.0.0\"\n  port: 8001\n  provider: \"funasr\"\n  language_code: \"zh\"\n  sample_rate: 16000\n  interim_transcript_min_samples: 16000\n  providers:\n    faster-whisper:\n      model_path: \"faster-whisper-small\"\n      device: null  # null = auto-detect\n      compute_type: null  # null = auto-detect\n    funasr:\n      model_name: \"paraformer-zh-streaming\"\n      vad_model: \"fsmn-vad\"\n      punc_model: \"ct-punc\"  # Optional punctuation model\n      vad_kwargs:\n        max_single_segment_time: 30000\n      device: null\n      batch_size_s: 0\n      streaming:\n        enabled: true\n        chunk_size: [0, 8, 4]\n        encoder_chunk_look_back: 4\n        decoder_chunk_look_back: 1\n        vad_chunk_size_ms: 100\n        silence_threshold_ms: 300\n</code></pre>"},{"location":"guide/configuration/#tts-configuration","title":"TTS Configuration","text":"<p>Supported: <code>genie-tts</code>, <code>edge-tts</code>, <code>chattts</code>, <code>elevenlabs</code>, <code>gpt-sovits</code>.</p> <pre><code>tts:\n  host: \"0.0.0.0\"\n  port: 8003\n  provider: \"genie-tts\"\n  providers:\n    edge-tts:\n      voice: \"zh-CN-XiaoyiNeural\"\n      rate: \"+0%\"\n      pitch: \"+0Hz\"\n    chattts:\n      model_source: \"local\"  # Options: local, huggingface, custom\n      device: null\n    elevenlabs:\n      voice_id: \"\"\n      stability: 0.5\n      similarity_boost: 0.8\n      style: 0.0\n    genie-tts:\n      character_name: \"ema\"\n      onnx_model_dir: \"path/to/model\"\n      language: \"jp\"  # Options: zh, en, jp\n      reference_audio_path: \"\"\n      reference_audio_text: \"\"\n      source_sample_rate: 32000\n    gpt-sovits:\n      server_url: \"http://127.0.0.1:9880\"\n      default_reference: \"default\"\n      default_text_lang: \"zh\"  # Default language for TTS input text\n      streaming_mode: 2  # 0=off, 1=best quality, 2=medium, 3=fastest\n      temperature: 1.0\n      top_p: 1.0\n      top_k: 15\n      speed_factor: 1.0\n      timeout: 30.0\n      references:\n        default:\n          ref_audio_path: \"references/default.wav\"\n          prompt_text: \"\u8fd9\u662f\u4e00\u4e2a\u9ed8\u8ba4\u7684\u53c2\u8003\u97f3\u9891\"\n          prompt_lang: \"zh\"\n        character1:\n          ref_audio_path: \"references/character1.wav\"\n          prompt_text: \"\u6211\u662f\u89d2\u82721\uff0c\u8bf4\u8bdd\u6e29\u67d4\u4f18\u96c5\"\n          prompt_lang: \"zh\"\n</code></pre>"},{"location":"guide/configuration/#audio-configuration","title":"Audio Configuration","text":"<pre><code>audio:\n  input:\n    sample_rate: 16000\n    channels: 1\n    device: null  # null = default device\n  output:\n    sample_rate: 32000\n    channels: 1\n    device: null  # null = default device\n  dtype: \"float32\"\n  block_size_ms: 100\n  silence_threshold_ms: 500  # Used as fallback for STT providers\n  listening_status_poll_interval: 1.0\n</code></pre>"},{"location":"guide/configuration/#obs-configuration","title":"OBS Configuration","text":"<pre><code>obs:\n  websocket:\n    host: \"localhost\"\n    port: 4455\n    password: \"\"\n  subtitle_source: \"subtitle\"\n  subtitle_ttl_seconds: 10\n  visibility_source: \"\"  # Optional\n  appear_filter_name: \"\"  # Optional\n  clear_filter_name: \"\"  # Optional\n</code></pre>"},{"location":"guide/configuration/#bilibili-configuration","title":"Bilibili Configuration","text":"<pre><code>bilibili:\n  enabled: true\n  room_id: 31232063\n  sessdata: \"\"  # Optional SESSDATA cookie\n  danmaku_ttl_seconds: 60\n</code></pre>"},{"location":"guide/configuration/#service-urls","title":"Service URLs","text":"<p>Configuration for inter-service communication (used by orchestrator).</p> <pre><code>services:\n  orchestrator_base_url: \"http://localhost:8000\"\n  stt_websocket_url: \"ws://localhost:8001/ws/transcribe\"\n  tts_websocket_url: \"ws://localhost:8003/synthesize/stream\"\n  ocr_websocket_url: \"ws://localhost:8004/monitor/stream\"\n  ocr_base_url: \"http://localhost:8004\"\n</code></pre>"},{"location":"guide/configuration/#hot-reloading","title":"Hot-Reloading","text":"<p>The Orchestrator monitors the <code>system_prompt_file</code> for changes. Updating the text file will automatically update the agent's persona without restarting.</p>"},{"location":"guide/installation/","title":"Installation Guide","text":"<p>The Voice Agent is designed to be easy to set up using <code>uv</code> or standard <code>pip</code>.</p>"},{"location":"guide/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"guide/installation/#1-python-environment","title":"1. Python Environment","text":"<ul> <li>Python 3.10 or higher is required.</li> <li>We recommend using uv for fast and reliable dependency management.</li> </ul>"},{"location":"guide/installation/#2-system-dependencies","title":"2. System Dependencies","text":"<ul> <li>PortAudio: Required for microphone capture.<ul> <li>Linux: <code>sudo apt install libportaudio2</code></li> <li>Windows/macOS: Usually included with Python audio libraries</li> </ul> </li> </ul>"},{"location":"guide/installation/#3-hardware-requirements","title":"3. Hardware Requirements","text":"<ul> <li>GPU: A CUDA-capable NVIDIA GPU is highly recommended for running local models (STT/LLM) with low latency. The more VRAM the better.</li> </ul>"},{"location":"guide/installation/#setup-steps","title":"Setup Steps","text":""},{"location":"guide/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/ndwang/voice_agent.git\ncd voice_agent\n</code></pre>"},{"location":"guide/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>The Voice Agent uses optional dependency groups, allowing you to install only the providers you need. This reduces installation size and avoids unnecessary dependencies.</p>"},{"location":"guide/installation/#minimal-installation-core-only","title":"Minimal Installation (Core Only)","text":"<p>For core functionality without any providers: <pre><code>uv sync\n</code></pre></p>"},{"location":"guide/installation/#install-all-providers","title":"Install All Providers","text":"<p>To install all optional providers and features: <pre><code>uv sync --extra all\n</code></pre></p>"},{"location":"guide/installation/#install-specific-providers","title":"Install Specific Providers","text":"<p>Install only the providers you plan to use:</p> <p>Example: FunASR STT + Ollama LLM + Edge TTS <pre><code>uv sync --extra stt-funasr --extra llm-ollama --extra tts-edge\n</code></pre></p>"},{"location":"guide/installation/#available-dependency-groups","title":"Available Dependency Groups","text":"<p>STT Providers: - <code>stt-faster-whisper</code> - Faster-Whisper STT provider - <code>stt-funasr</code> - FunASR STT provider - <code>stt-all</code> - All STT providers</p> <p>LLM Providers: - <code>llm-gemini</code> - Google Gemini API provider - <code>llm-ollama</code> - Ollama local LLM provider - <code>llm-llama-cpp</code> - llama.cpp provider - <code>llm-all</code> - All LLM providers</p> <p>TTS Providers: - <code>tts-edge</code> - Edge TTS (Microsoft Edge voices) - <code>tts-elevenlabs</code> - ElevenLabs TTS provider - <code>tts-genie</code> - Genie TTS provider - <code>tts-all</code> - All TTS providers</p> <p>Other: - <code>ocr</code> - OCR dependencies (PaddleOCR, etc.) - <code>all</code> - JUST GIVE ME EVERYTHING (STT, LLM, TTS, OCR)</p>"},{"location":"guide/installation/#gpu-and-cuda-configuration","title":"GPU and CUDA Configuration","text":"<p>The default installation assumes CUDA 12.6. If you have a different CUDA version or wish to run without a GPU, you may need to modify <code>pyproject.toml</code>.</p> <p>Customizing CUDA Version</p> <p>If your system uses a different CUDA version (e.g., 12.4), update the <code>pytorch-cuda</code> index in <code>pyproject.toml</code>:</p> <pre><code>[[tool.uv.index]]\nname = \"pytorch-cuda\"\n# For CUDA 12.4, use: https://download.pytorch.org/whl/cu124\n# For CUDA 11.8, use: https://download.pytorch.org/whl/cu118\nurl = \"https://download.pytorch.org/whl/cu126\" \nexplicit = true\n</code></pre> <p>After modifying the file, run <code>uv sync</code> again.</p> <p>CPU-Only Installation</p> <p>If you do not have an NVIDIA GPU, you can remove or comment out the <code>[tool.uv.sources]</code> and <code>[[tool.uv.index]]</code> sections in <code>pyproject.toml</code> to install the standard CPU versions of Torch.</p>"},{"location":"guide/installation/#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer pip, you can install with optional dependencies: <pre><code>python -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\npip install -e \".[all]\"  # Install all providers\n# Or install specific providers:\npip install -e \".[stt-faster-whisper,llm-ollama,tts-edge]\"\n</code></pre></p>"},{"location":"guide/installation/#3-llm-setup-local","title":"3. LLM Setup (Local)","text":"<p>If you plan to use Ollama: 1. Install Ollama from ollama.com. 2. Pull your desired model:    <pre><code>ollama pull Qwen3-8B  # or any other supported model\n</code></pre></p>"},{"location":"guide/installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Run the GPU verification script: <pre><code>python scripts/verify_gpu_setup.py\n</code></pre></p> <p>Check if audio devices are detected: <pre><code>python scripts/list_output_devices.py\n</code></pre></p>"},{"location":"guide/installation/#additional-setup","title":"Additional Setup","text":""},{"location":"guide/installation/#bilibili-integration-blivedm","title":"Bilibili Integration (blivedm)","text":"<p>To use the Bilibili live stream source, you must install the <code>blivedm</code> library. Note: The version on PyPI is abandoned and incompatible; you must install it directly from the source.</p> <ul> <li>Installation (Remote): <code>uv pip install git+https://github.com/xfgryujk/blivedm.git</code></li> <li>Project Page: xfgryujk/blivedm</li> </ul>"},{"location":"guide/installation/#chattts","title":"ChatTTS","text":"<p>To use the <code>chattts</code> provider for TTS, it must be installed separately (not available as an optional dependency): - Project Page: 2noise/ChatTTS</p>"},{"location":"guide/installation/#provider-specific-notes","title":"Provider-Specific Notes","text":"<p>Genie TTS: - Requires character-specific ONNX models. See the Genie TTS page for details on model preparation.</p> <p>Edge-TTS: - Requires <code>ffmpeg</code> installed on your system path:     - Windows: <code>choco install ffmpeg</code> or download from ffmpeg.org.     - Linux: <code>sudo apt install ffmpeg</code>     - macOS: <code>brew install ffmpeg</code> - Project Page: rany2/edge-tts</p>"},{"location":"services/audio_player/","title":"Audio Player Service","text":"<p>The Audio Player provides low-latency streaming audio playback for the voice agent system. It is designed to play audio chunks in real-time as they are generated by the TTS service, enabling responsive voice interactions.</p>"},{"location":"services/audio_player/#features","title":"Features","text":"<ul> <li>Streaming Playback: Plays audio chunks as they arrive, without waiting for the complete audio stream.</li> <li>Low Latency: Optimized for minimal delay between audio generation and playback (typically ~50ms buffer).</li> <li>Automatic Resampling: Handles sample rate conversion between TTS output and playback device requirements.</li> <li>Device Selection: Supports custom audio output device selection via configuration.</li> <li>State Management: Provides callbacks for play state changes, enabling coordination with other system components.</li> </ul>"},{"location":"services/audio_player/#architecture","title":"Architecture","text":"<p>The AudioPlayer uses a multi-threaded architecture:</p> <ol> <li>Main Thread: Receives audio chunks via <code>play_audio_chunk()</code> and queues them for processing.</li> <li>Playback Loop: An async task that processes the queue, resamples audio, and feeds a buffer.</li> <li>Audio Thread: A separate thread (managed by sounddevice) that continuously reads from the buffer and outputs to the audio device.</li> </ol> <p>This design ensures that audio playback is never blocked by other operations, and incoming audio chunks can be processed immediately.</p>"},{"location":"services/audio_player/#configuration","title":"Configuration","text":"<p>Audio Player settings are configured in <code>config.yaml</code> under the <code>audio.output</code> section:</p> <pre><code>audio:\n  output:\n    sample_rate: 32000  # Playback sample rate (Hz)\n    channels: 1         # Number of audio channels (1 = mono, 2 = stereo)\n    device: null        # Output device index or name (null = system default)\n</code></pre>"},{"location":"services/audio_player/#sample-rate","title":"Sample Rate","text":"<p>The playback sample rate should match your audio device's capabilities. Common values: - 16000 Hz: Standard for voice/speech - 32000 Hz: Higher quality for voice - 44100 Hz: CD quality - 48000 Hz: Professional audio standard</p>"},{"location":"services/audio_player/#device-selection","title":"Device Selection","text":"<p>To use a specific audio output device:</p> <ol> <li> <p>List available devices using the provided script:    <pre><code>uv run scripts/list_output_devices.py\n</code></pre></p> </li> <li> <p>Set the device in <code>config.yaml</code>:</p> </li> <li>By index: <code>device: 1</code></li> <li>By name: <code>device: \"Speakers (Realtek Audio)\"</code></li> <li>System default: <code>device: null</code></li> </ol>"},{"location":"services/audio_player/#usage","title":"Usage","text":""},{"location":"services/audio_player/#basic-playback","title":"Basic Playback","text":"<pre><code>from audio.audio_player import AudioPlayer\n\n# Initialize player\nplayer = AudioPlayer()\n\n# Play audio chunk (bytes, float32 format, normalized to [-1, 1])\nawait player.play_audio_chunk(audio_bytes, source_sample_rate=24000)\n\n# Stop playback\nawait player.stop()\n</code></pre>"},{"location":"services/audio_player/#with-play-state-callback","title":"With Play State Callback","text":"<p>The AudioPlayer can notify you when playback starts or stops:</p> <pre><code>async def on_play_state_changed(is_playing: bool):\n    if is_playing:\n        print(\"Audio playback started\")\n    else:\n        print(\"Audio playback stopped\")\n\nplayer = AudioPlayer(on_play_state=on_play_state_changed)\n</code></pre> <p>This is useful for coordinating with other system components, such as: - Disabling microphone input while speaking - Updating UI indicators - Managing interaction state</p>"},{"location":"services/audio_player/#audio-format","title":"Audio Format","text":""},{"location":"services/audio_player/#input-format","title":"Input Format","text":"<ul> <li>Data Type: <code>bytes</code> containing <code>float32</code> samples</li> <li>Normalization: Samples should be in the range <code>[-1.0, 1.0]</code></li> <li>Layout: Interleaved samples (for multi-channel audio)</li> <li>Sample Rate: Any rate supported by the source (TTS service typically outputs 24000 Hz)</li> </ul>"},{"location":"services/audio_player/#processing-pipeline","title":"Processing Pipeline","text":"<ol> <li>Queue: Incoming chunks are added to an async queue</li> <li>Conversion: Bytes are converted to NumPy arrays</li> <li>Resampling: Audio is resampled to the playback sample rate (if needed)</li> <li>Buffering: Resampled audio is added to a thread-safe buffer</li> <li>Playback: The audio device callback reads from the buffer in real-time</li> </ol>"},{"location":"services/audio_player/#resampling","title":"Resampling","text":"<p>The AudioPlayer automatically handles sample rate conversion:</p> <ul> <li>High Quality: Uses <code>scipy.signal.resample()</code> if scipy is available</li> <li>Fallback: Uses linear interpolation if scipy is not installed</li> </ul> <p>Resampling is performed on-the-fly, so there's no need to pre-convert audio to the playback sample rate.</p>"},{"location":"services/audio_player/#latency-optimization","title":"Latency Optimization","text":"<p>The AudioPlayer is optimized for low latency:</p> <ul> <li>Small Buffer: ~50ms buffer size minimizes delay</li> <li>Streaming: Audio starts playing as soon as the first chunk arrives</li> <li>No Pre-buffering: The system doesn't wait for complete audio before starting playback</li> </ul> <p>This enables natural conversation flow, as the agent can start speaking almost immediately after generating the first audio chunk.</p>"},{"location":"services/audio_player/#thread-safety","title":"Thread Safety","text":"<p>The AudioPlayer is designed to be thread-safe:</p> <ul> <li>Queue Operations: The async queue handles concurrent access safely</li> <li>Buffer Access: Thread locks protect the audio buffer during read/write operations</li> <li>State Management: Play state changes are synchronized via the event loop</li> </ul> <p>You can safely call <code>play_audio_chunk()</code> from any async context without additional synchronization.</p>"},{"location":"services/audio_player/#error-handling","title":"Error Handling","text":"<p>The AudioPlayer includes robust error handling:</p> <ul> <li>Device Errors: Logs warnings if the audio device reports issues</li> <li>Processing Errors: Catches and logs exceptions during audio processing</li> <li>Stream Errors: Gracefully handles stream closure and cleanup</li> </ul> <p>If an error occurs, the playback loop will stop and clean up resources, but the player can be reused by calling <code>play_audio_chunk()</code> again.</p>"},{"location":"services/audio_player/#integration-with-tts-service","title":"Integration with TTS Service","text":"<p>The AudioPlayer is typically used by the Orchestrator's <code>TTSManager</code> to play audio received from the TTS service:</p> <ol> <li>TTS service generates audio chunks via WebSocket</li> <li>TTSManager receives chunks and calls <code>player.play_audio_chunk()</code></li> <li>AudioPlayer streams the audio to the output device</li> <li>Play state callbacks notify the system when playback starts/stops</li> </ol> <p>This integration enables seamless voice interactions with minimal latency.</p>"},{"location":"services/stt/","title":"STT Service (Speech-to-Text)","text":"<p>The STT Service provides real-time transcription of audio streams. It is designed to run as a standalone service (on port 8001) that multiple clients can connect to via WebSocket.</p>"},{"location":"services/stt/#supported-providers","title":"Supported Providers","text":""},{"location":"services/stt/#1-funasr-recommended","title":"1. FunASR (Recommended)","text":"<p>Uses Alibaba's FunASR framework. It is highly optimized for Chinese and English, supporting: - Streaming Paraformer: Low-latency transcription. - VAD (Voice Activity Detection): Built-in model to detect speech boundaries. - Punctuation: Automatic punctuation restoration.</p>"},{"location":"services/stt/#2-faster-whisper","title":"2. Faster-Whisper","text":"<p>An efficient implementation of OpenAI's Whisper model using CTranslate2.  - Best for high-accuracy multilingual support. - Supports different model sizes (tiny, base, small, medium, large).</p>"},{"location":"services/stt/#websocket-protocol","title":"WebSocket Protocol","text":""},{"location":"services/stt/#input-audio-streaming","title":"Input: Audio Streaming","text":"<p>The client sends raw PCM audio bytes (typically 16kHz, mono, float32) directly over the WebSocket.</p>"},{"location":"services/stt/#output-json-transcripts","title":"Output: JSON Transcripts","text":"<p>The service broadcasts transcripts to all connected clients:</p> <pre><code>// Interim (Partial) Result\n{\n  \"type\": \"interim\",\n  \"text\": \"Hello world\"\n}\n\n// Final (Sentence) Result\n{\n  \"type\": \"final\",\n  \"text\": \"Hello world.\"\n}\n\n// Speech Boundary Detection\n{\n  \"type\": \"speech_start\"\n}\n</code></pre>"},{"location":"services/stt/#internal-workflow","title":"Internal Workflow","text":"<ol> <li>VAD Chunking: Incoming audio is processed by a VAD model to identify speech segments.</li> <li>Model Inference: Speech chunks are sent to the ASR model.</li> <li>Post-Processing: (Optional) Punctuation and text formatting are applied.</li> <li>Broadcast: The resulting text is sent back through the WebSocket.</li> </ol>"},{"location":"services/tts/","title":"TTS Service (Text-to-Speech)","text":"<p>The TTS Service (port 8003) converts text into natural-sounding speech. It supports both high-quality offline models and fast online providers.</p>"},{"location":"services/tts/#supported-providers","title":"Supported Providers","text":""},{"location":"services/tts/#1-genie-tts-high-quality","title":"1. Genie-TTS (High Quality)","text":"<p>A high-performance CPU inference engine for GPT-SoVITS models.  - Character Consistency: Uses pre-trained character models. - Language Support: Optimized for Japanese, Chinese, and English. - Latency: GENIE optimizes the original model for outstanding CPU performance.</p>"},{"location":"services/tts/#2-edge-tts-fast-easy","title":"2. Edge-TTS (Fast &amp; Easy)","text":"<p>Uses Microsoft Edge's online TTS engine. - No GPU Required: Processing happens on Microsoft servers. - Wide Voice Selection: Access to dozens of neural voices. - Requirement: <code>ffmpeg</code> must be installed locally for format conversion.</p>"},{"location":"services/tts/#3-chattts","title":"3. ChatTTS","text":"<p>Optimized for conversational speech, including fillers like [laugh] and [um].</p>"},{"location":"services/tts/#websocket-streaming-protocol","title":"WebSocket Streaming Protocol","text":"<p>To minimize \"Time to First Byte\" (TTFB), the TTS Service supports streaming synthesis:</p> <ol> <li>Client Request:    <pre><code>{\n  \"type\": \"text\",\n  \"text\": \"Hello, how are you?\",\n  \"finalize\": false\n}\n</code></pre></li> <li>Audio Streaming: The service begins generating audio and sends it back as raw binary PCM chunks.</li> <li>Finalization: When <code>finalize: true</code> is sent, the service finishes the current buffer and closes the stream segment.</li> </ol>"},{"location":"services/tts/#latency-optimization","title":"Latency Optimization","text":"<ul> <li>Pre-connection: The Orchestrator's <code>TTSManager</code> opens a WebSocket connection as soon as the LLM starts generating, reducing the handshake overhead when the first sentence is ready.</li> <li>Sentence-Level Chunking: The system doesn't wait for the full LLM response. As soon as a sentence is complete (detected by punctuation or tags), it is sent to the TTS service.</li> </ul>"}]}