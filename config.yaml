# Voice Agent Configuration File
# All submodules use this file for configuration.

# Orchestrator Service Configuration
orchestrator:
  host: "0.0.0.0"
  port: 8000
  stt_websocket_path: "/ws/stt"
  log_level: "INFO"
  enable_latency_tracking: true
  # system_prompt_file: "orchestrator/system_prompt.txt"  # Optional: path to system prompt file (default: orchestrator/system_prompt.txt)
  # system_prompt_reload_interval: 1.0  # Optional: seconds between file checks for hot-reload (default: 1.0)
  hotkeys:
    toggle_listening: "ctrl+shift+l"  # Default hotkey for toggling listening

# STT (Speech-to-Text) Service Configuration
stt:
  host: "0.0.0.0"
  port: 8001
  provider: "funasr"  # Options: faster-whisper, funasr
  language_code: "zh"  # Language code for transcription
  sample_rate: 16000
  interim_transcript_min_samples: 16000  # Minimum audio samples (1s at 16kHz) for interim transcription
  flush_command: "\x00"  # Special byte command to finalize transcription
  providers:
    faster-whisper:
      model_path: "faster-whisper-small"  # relative to stt/
      device: null  # null = auto-detect (cuda if available, else cpu)
      compute_type: null  # null = auto-detect (int8 if cuda, else default)
    funasr:
      model_name: "paraformer-zh-streaming" #"FunAudioLLM/Fun-ASR-Nano-2512"
      vad_model: "fsmn-vad"  # VAD model name
      vad_kwargs:
        max_single_segment_time: 30000  # Max segment time in ms
      punc_model: "ct-punc"  # Punctuation model name (e.g., "ct-punc" or "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch")
      device: null  # null = auto-detect (cpu), or "cuda:0", "cuda", "cpu"
      batch_size_s: 0  # Batch size for processing
      streaming:
        enabled: true  # Enable streaming mode
        chunk_size: [0, 8, 4]  # ASR chunk configuration [0, 8, 4] = 480ms
        encoder_chunk_look_back: 4  # Number of chunks to lookback for encoder self-attention
        decoder_chunk_look_back: 1  # Number of encoder chunks to lookback for decoder cross-attention
        vad_chunk_size_ms: 100  # VAD processing chunk size in milliseconds
        silence_threshold_ms: 300  # Silence duration threshold to trigger finalization (ms)

# LLM (Language Model) Configuration
llm:
  provider: "ollama"  # Options: gemini, ollama
  providers:
    gemini:
      model: "gemini-2.5-flash"  # Model name
      api_key: ""  # Optional: Override GEMINI_API_KEY environment variable. Leave empty to use env var.
    ollama:
      model: "Qwen3-8B-Q4-8kcontext"  # Model name from `ollama list`
      base_url: "http://localhost:11434"  # Default Ollama URL
      timeout: 300.0  # Request timeout in seconds
      disable_thinking: true  # Set to true to disable thinking/reasoning mode in models that support it

# TTS (Text-to-Speech) Service Configuration
tts:
  host: "0.0.0.0"
  port: 8003
  provider: "edge-tts"  # Options: edge-tts, chattts
  output_sample_rate: null  # When null, inherits audio.output.sample_rate
  providers:
    edge-tts:
      voice: "zh-CN-XiaoyiNeural"  # Voice name
      rate: "+0%"  # Speech rate (e.g., -50% for slower, +50% for faster)
      pitch: "+0Hz"  # Pitch adjustment
    chattts:
      model_source: "local"  # Options: local, huggingface, custom
      device: null  # Options: cuda, cpu, null (auto-detect)

# OCR (Optical Character Recognition) Service Configuration
ocr:
  host: "0.0.0.0"
  port: 8004
  language: "ch"  # Chinese and English support
  interval_ms: 1000  # Default monitoring interval in milliseconds
  texts_storage_file_prefix: "ocr_detected_texts"  # Prefix for text storage files

# Audio Driver Configuration
audio:
  input:
    sample_rate: 16000
    channels: 1
    device: null  # Set to device index or name, null for default
  output:
    sample_rate: 16000  # Playback sample rate and TTS target (e.g., 48000)
    channels: 1
    device: null #31  # Set to device index or name, null for default
  dtype: "float32"
  block_size_ms: 100  # Determines VAD sensitivity
  flush_command: "\x00"  # Special byte command to finalize transcription
  silence_threshold_ms: 500  # 500ms of silence triggers a flush
  vad_min_speech_prob: 0.5  # Minimum speech probability for VAD
  vad_lookback_ms: 500  # Amount of audio to buffer before speech detection (prevents missing sentence beginnings)
  plot_window_seconds: 2  # Show last 2 seconds of audio in plot
  plot_update_interval_ms: 50  # Update plot every 50ms
  listening_status_poll_interval: 1.0  # Seconds between polling orchestrator for listening status

# OBS Websocket Configuration
obs:
  websocket:
    host: "localhost"
    port: 4455
    password: "j22KXyZ1ONOk5m70"
  subtitle_source: "subtitle"  # Name of the text source in OBS for subtitles

# Service URLs (used by orchestrator to connect to other services)
services:
  orchestrator_base_url: "http://localhost:8000"  # Base URL for orchestrator API (used by audio driver)
  stt_websocket_url: "ws://localhost:8001/ws/transcribe"
  tts_websocket_url: "ws://localhost:8003/synthesize/stream"
  ocr_websocket_url: "ws://localhost:8004/monitor/stream"
  ocr_base_url: "http://localhost:8004"

